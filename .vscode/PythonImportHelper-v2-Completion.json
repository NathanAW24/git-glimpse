[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "evaluate",
        "description": "evaluate",
        "detail": "evaluate",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "openai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openai",
        "description": "openai",
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "wrap",
        "importPath": "textwrap",
        "description": "textwrap",
        "isExtraImport": true,
        "detail": "textwrap",
        "documentation": {}
    },
    {
        "label": "wrap",
        "importPath": "textwrap",
        "description": "textwrap",
        "isExtraImport": true,
        "detail": "textwrap",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_chroma",
        "description": "langchain_chroma",
        "isExtraImport": true,
        "detail": "langchain_chroma",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_chroma",
        "description": "langchain_chroma",
        "isExtraImport": true,
        "detail": "langchain_chroma",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_chroma",
        "description": "langchain_chroma",
        "isExtraImport": true,
        "detail": "langchain_chroma",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_chroma",
        "description": "langchain_chroma",
        "isExtraImport": true,
        "detail": "langchain_chroma",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_chroma",
        "description": "langchain_chroma",
        "isExtraImport": true,
        "detail": "langchain_chroma",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_chroma",
        "description": "langchain_chroma",
        "isExtraImport": true,
        "detail": "langchain_chroma",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain_huggingface",
        "description": "langchain_huggingface",
        "isExtraImport": true,
        "detail": "langchain_huggingface",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain_huggingface",
        "description": "langchain_huggingface",
        "isExtraImport": true,
        "detail": "langchain_huggingface",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain_huggingface",
        "description": "langchain_huggingface",
        "isExtraImport": true,
        "detail": "langchain_huggingface",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain_huggingface",
        "description": "langchain_huggingface",
        "isExtraImport": true,
        "detail": "langchain_huggingface",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain_huggingface",
        "description": "langchain_huggingface",
        "isExtraImport": true,
        "detail": "langchain_huggingface",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain_huggingface",
        "description": "langchain_huggingface",
        "isExtraImport": true,
        "detail": "langchain_huggingface",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain_huggingface",
        "description": "langchain_huggingface",
        "isExtraImport": true,
        "detail": "langchain_huggingface",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain_core.runnables",
        "description": "langchain_core.runnables",
        "isExtraImport": true,
        "detail": "langchain_core.runnables",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain_core.runnables",
        "description": "langchain_core.runnables",
        "isExtraImport": true,
        "detail": "langchain_core.runnables",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain_core.runnables",
        "description": "langchain_core.runnables",
        "isExtraImport": true,
        "detail": "langchain_core.runnables",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain_core.runnables",
        "description": "langchain_core.runnables",
        "isExtraImport": true,
        "detail": "langchain_core.runnables",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain_core.runnables",
        "description": "langchain_core.runnables",
        "isExtraImport": true,
        "detail": "langchain_core.runnables",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "BM25Retriever",
        "importPath": "index",
        "description": "index",
        "isExtraImport": true,
        "detail": "index",
        "documentation": {}
    },
    {
        "label": "_read_documents_from_folder",
        "importPath": "index",
        "description": "index",
        "isExtraImport": true,
        "detail": "index",
        "documentation": {}
    },
    {
        "label": "BM25Okapi",
        "importPath": "rank_bm25",
        "description": "rank_bm25",
        "isExtraImport": true,
        "detail": "rank_bm25",
        "documentation": {}
    },
    {
        "label": "BM25Okapi",
        "importPath": "rank_bm25",
        "description": "rank_bm25",
        "isExtraImport": true,
        "detail": "rank_bm25",
        "documentation": {}
    },
    {
        "label": "BM25Okapi",
        "importPath": "rank_bm25",
        "description": "rank_bm25",
        "isExtraImport": true,
        "detail": "rank_bm25",
        "documentation": {}
    },
    {
        "label": "BM25Okapi",
        "importPath": "rank_bm25",
        "description": "rank_bm25",
        "isExtraImport": true,
        "detail": "rank_bm25",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "BM25Retriever",
        "importPath": "bm25",
        "description": "bm25",
        "isExtraImport": true,
        "detail": "bm25",
        "documentation": {}
    },
    {
        "label": "_read_documents_from_folder",
        "importPath": "bm25",
        "description": "bm25",
        "isExtraImport": true,
        "detail": "bm25",
        "documentation": {}
    },
    {
        "label": "BM25Retriever",
        "importPath": "bm25",
        "description": "bm25",
        "isExtraImport": true,
        "detail": "bm25",
        "documentation": {}
    },
    {
        "label": "_read_documents_from_folder",
        "importPath": "bm25",
        "description": "bm25",
        "isExtraImport": true,
        "detail": "bm25",
        "documentation": {}
    },
    {
        "label": "BM25Retriever",
        "importPath": "bm25",
        "description": "bm25",
        "isExtraImport": true,
        "detail": "bm25",
        "documentation": {}
    },
    {
        "label": "_read_documents_from_folder",
        "importPath": "bm25",
        "description": "bm25",
        "isExtraImport": true,
        "detail": "bm25",
        "documentation": {}
    },
    {
        "label": "CrossEncoder",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "CrossEncoder",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "streamlit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "streamlit",
        "description": "streamlit",
        "detail": "streamlit",
        "documentation": {}
    },
    {
        "label": "EnsembleRetriever",
        "importPath": "retriever",
        "description": "retriever",
        "isExtraImport": true,
        "detail": "retriever",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "call_llm",
        "kind": 2,
        "importPath": "10_summarization_eval.iterative_summary",
        "description": "10_summarization_eval.iterative_summary",
        "peekOfCode": "def call_llm(system_prompt, user_prompt, model=\"gpt-4o\", temperature=0.7):\n    \"\"\"Generic LLM call.\"\"\"\n    response = openai.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=temperature\n    )",
        "detail": "10_summarization_eval.iterative_summary",
        "documentation": {}
    },
    {
        "label": "read_txt_files",
        "kind": 2,
        "importPath": "10_summarization_eval.iterative_summary",
        "description": "10_summarization_eval.iterative_summary",
        "peekOfCode": "def read_txt_files(folder_path):\n    files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n    documents = []\n    for file in files:\n        with open(os.path.join(folder_path, file), \"r\") as f:\n            documents.append(f.read())\n    return documents\n# Function to summarize using a specific GPT model\ndef summarize_with_gpt(model, text):\n    system_prompt = \"You are a helpful assistant specializing in summarization. Your summary need to be detailed as this is for coding. If there is code, make sure you include it. Your summary must be long and detailed\"",
        "detail": "10_summarization_eval.iterative_summary",
        "documentation": {}
    },
    {
        "label": "summarize_with_gpt",
        "kind": 2,
        "importPath": "10_summarization_eval.iterative_summary",
        "description": "10_summarization_eval.iterative_summary",
        "peekOfCode": "def summarize_with_gpt(model, text):\n    system_prompt = \"You are a helpful assistant specializing in summarization. Your summary need to be detailed as this is for coding. If there is code, make sure you include it. Your summary must be long and detailed\"\n    user_prompt = f\"Summarize the following text:\\n{text}\"\n    summary = call_llm(system_prompt, user_prompt,\n                       model=model, temperature=0.7)\n    return summary.strip()\n# Two-step partial-pass summarization\ndef two_step_partial_pass(model, text, chunk_size=1500):\n    \"\"\"Divide the text into chunks, summarize each chunk, and combine the summaries.\"\"\"\n    # Split text into smaller chunks",
        "detail": "10_summarization_eval.iterative_summary",
        "documentation": {}
    },
    {
        "label": "two_step_partial_pass",
        "kind": 2,
        "importPath": "10_summarization_eval.iterative_summary",
        "description": "10_summarization_eval.iterative_summary",
        "peekOfCode": "def two_step_partial_pass(model, text, chunk_size=1500):\n    \"\"\"Divide the text into chunks, summarize each chunk, and combine the summaries.\"\"\"\n    # Split text into smaller chunks\n    chunks = wrap(text, chunk_size)\n    # Summarize each chunk\n    chunk_summaries = []\n    for i, chunk in enumerate(chunks):\n        print(\n            f\"Summarizing chunk {i+1}/{len(chunks)} with two-step partial-pass...\")\n        chunk_summary = summarize_with_gpt(model, chunk)",
        "detail": "10_summarization_eval.iterative_summary",
        "documentation": {}
    },
    {
        "label": "evaluate_summaries_harimplus",
        "kind": 2,
        "importPath": "10_summarization_eval.iterative_summary",
        "description": "10_summarization_eval.iterative_summary",
        "peekOfCode": "def evaluate_summaries_harimplus(articles, summaries):\n    scorer = evaluate.load(\"NCSOFT/harim_plus\")\n    scores = scorer.compute(predictions=summaries, references=articles)\n    return scores\n######################################################\n# ADDITIONAL CODE FOR ITERATIVE REFINEMENT SUMMARIZATION\n######################################################\ndef iterative_refinement_summarization(model, text, chunk_size=4000):\n    \"\"\"\n    Iteratively refine a summary by:",
        "detail": "10_summarization_eval.iterative_summary",
        "documentation": {}
    },
    {
        "label": "iterative_refinement_summarization",
        "kind": 2,
        "importPath": "10_summarization_eval.iterative_summary",
        "description": "10_summarization_eval.iterative_summary",
        "peekOfCode": "def iterative_refinement_summarization(model, text, chunk_size=4000):\n    \"\"\"\n    Iteratively refine a summary by:\n    1. Summarizing the first chunk.\n    2. For each subsequent chunk, refine the summary using the new context.\n    \"\"\"\n    # Split text into chunks\n    chunks = wrap(text, chunk_size)\n    if not chunks:\n        return summarize_with_gpt(model, text)",
        "detail": "10_summarization_eval.iterative_summary",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "10_summarization_eval.iterative_summary",
        "description": "10_summarization_eval.iterative_summary",
        "peekOfCode": "def main():\n    folder_path = \"processed_docs\"  # Folder containing the .txt files\n    output_path = \"summarization_results.json\"  # JSON file to save results\n    # Read the documents\n    documents = read_txt_files(folder_path)\n    if not documents:\n        print(\"No documents found in the specified folder.\")\n        return\n    # Prepare models\n    gpt_models = [\"gpt-4o-mini\"]",
        "detail": "10_summarization_eval.iterative_summary",
        "documentation": {}
    },
    {
        "label": "openai.api_key",
        "kind": 5,
        "importPath": "10_summarization_eval.iterative_summary",
        "description": "10_summarization_eval.iterative_summary",
        "peekOfCode": "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n# Generic LLM call function\ndef call_llm(system_prompt, user_prompt, model=\"gpt-4o\", temperature=0.7):\n    \"\"\"Generic LLM call.\"\"\"\n    response = openai.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],",
        "detail": "10_summarization_eval.iterative_summary",
        "documentation": {}
    },
    {
        "label": "call_llm",
        "kind": 2,
        "importPath": "10_summarization_eval.one_step_full_pass",
        "description": "10_summarization_eval.one_step_full_pass",
        "peekOfCode": "def call_llm(system_prompt, user_prompt, model=\"gpt-4o\", temperature=0.7):\n    \"\"\"Generic LLM call.\"\"\"\n    response = openai.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=temperature\n    )",
        "detail": "10_summarization_eval.one_step_full_pass",
        "documentation": {}
    },
    {
        "label": "read_txt_files",
        "kind": 2,
        "importPath": "10_summarization_eval.one_step_full_pass",
        "description": "10_summarization_eval.one_step_full_pass",
        "peekOfCode": "def read_txt_files(folder_path):\n    files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n    documents = []\n    for file in files:\n        with open(os.path.join(folder_path, file), \"r\") as f:\n            documents.append(f.read())\n    return documents\n# Function to summarize using a specific GPT model\ndef summarize_with_gpt(model, text):\n    system_prompt = \"You are a helpful assistant specializing in summarization. Your summary need to be detailed as this is for coding. If there is code, make sure you include it. Your summary must be long and detailed\"",
        "detail": "10_summarization_eval.one_step_full_pass",
        "documentation": {}
    },
    {
        "label": "summarize_with_gpt",
        "kind": 2,
        "importPath": "10_summarization_eval.one_step_full_pass",
        "description": "10_summarization_eval.one_step_full_pass",
        "peekOfCode": "def summarize_with_gpt(model, text):\n    system_prompt = \"You are a helpful assistant specializing in summarization. Your summary need to be detailed as this is for coding. If there is code, make sure you include it. Your summary must be long and detailed\"\n    user_prompt = f\"Summarize the following text:\\n{text}\"\n    summary = call_llm(system_prompt, user_prompt,\n                       model=model, temperature=0.7)\n    return summary.strip()\n# Function to evaluate summaries using HaRiM+\ndef evaluate_summaries_harimplus(articles, summaries):\n    scorer = evaluate.load(\"NCSOFT/harim_plus\")\n    scores = scorer.compute(predictions=summaries, references=articles)",
        "detail": "10_summarization_eval.one_step_full_pass",
        "documentation": {}
    },
    {
        "label": "evaluate_summaries_harimplus",
        "kind": 2,
        "importPath": "10_summarization_eval.one_step_full_pass",
        "description": "10_summarization_eval.one_step_full_pass",
        "peekOfCode": "def evaluate_summaries_harimplus(articles, summaries):\n    scorer = evaluate.load(\"NCSOFT/harim_plus\")\n    scores = scorer.compute(predictions=summaries, references=articles)\n    return scores\n# Main script\ndef main():\n    folder_path = \"processed_docs\"  # Folder containing the .txt files\n    output_path = \"summarization_results.json\"  # JSON file to save results\n    # Read the documents\n    documents = read_txt_files(folder_path)",
        "detail": "10_summarization_eval.one_step_full_pass",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "10_summarization_eval.one_step_full_pass",
        "description": "10_summarization_eval.one_step_full_pass",
        "peekOfCode": "def main():\n    folder_path = \"processed_docs\"  # Folder containing the .txt files\n    output_path = \"summarization_results.json\"  # JSON file to save results\n    # Read the documents\n    documents = read_txt_files(folder_path)\n    if not documents:\n        print(\"No documents found in the specified folder.\")\n        return\n    # Prepare models\n    gpt_models = [\"gpt-4o-mini\"]",
        "detail": "10_summarization_eval.one_step_full_pass",
        "documentation": {}
    },
    {
        "label": "openai.api_key",
        "kind": 5,
        "importPath": "10_summarization_eval.one_step_full_pass",
        "description": "10_summarization_eval.one_step_full_pass",
        "peekOfCode": "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n# Generic LLM call function\ndef call_llm(system_prompt, user_prompt, model=\"gpt-4o\", temperature=0.7):\n    \"\"\"Generic LLM call.\"\"\"\n    response = openai.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],",
        "detail": "10_summarization_eval.one_step_full_pass",
        "documentation": {}
    },
    {
        "label": "call_llm",
        "kind": 2,
        "importPath": "10_summarization_eval.two_step_partial_pass",
        "description": "10_summarization_eval.two_step_partial_pass",
        "peekOfCode": "def call_llm(system_prompt, user_prompt, model=\"gpt-4o\", temperature=0.7):\n    \"\"\"Generic LLM call.\"\"\"\n    response = openai.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=temperature\n    )",
        "detail": "10_summarization_eval.two_step_partial_pass",
        "documentation": {}
    },
    {
        "label": "read_txt_files",
        "kind": 2,
        "importPath": "10_summarization_eval.two_step_partial_pass",
        "description": "10_summarization_eval.two_step_partial_pass",
        "peekOfCode": "def read_txt_files(folder_path):\n    files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n    documents = []\n    for file in files:\n        with open(os.path.join(folder_path, file), \"r\") as f:\n            documents.append(f.read())\n    return documents\n# Function to summarize using a specific GPT model\ndef summarize_with_gpt(model, text):\n    system_prompt = \"You are a helpful assistant specializing in summarization. Your summary need to be detailed as this is for coding. If there is code, make sure you include it. Your summary must be long and detailed\"",
        "detail": "10_summarization_eval.two_step_partial_pass",
        "documentation": {}
    },
    {
        "label": "summarize_with_gpt",
        "kind": 2,
        "importPath": "10_summarization_eval.two_step_partial_pass",
        "description": "10_summarization_eval.two_step_partial_pass",
        "peekOfCode": "def summarize_with_gpt(model, text):\n    system_prompt = \"You are a helpful assistant specializing in summarization. Your summary need to be detailed as this is for coding. If there is code, make sure you include it. Your summary must be long and detailed\"\n    user_prompt = f\"Summarize the following text:\\n{text}\"\n    summary = call_llm(system_prompt, user_prompt,\n                       model=model, temperature=0.7)\n    return summary.strip()\n# Two-step partial-pass summarization\ndef two_step_partial_pass(model, text, chunk_size=3000):\n    \"\"\"Divide the text into chunks, summarize each chunk, and combine the summaries.\"\"\"\n    # Split text into smaller chunks",
        "detail": "10_summarization_eval.two_step_partial_pass",
        "documentation": {}
    },
    {
        "label": "two_step_partial_pass",
        "kind": 2,
        "importPath": "10_summarization_eval.two_step_partial_pass",
        "description": "10_summarization_eval.two_step_partial_pass",
        "peekOfCode": "def two_step_partial_pass(model, text, chunk_size=3000):\n    \"\"\"Divide the text into chunks, summarize each chunk, and combine the summaries.\"\"\"\n    # Split text into smaller chunks\n    chunks = wrap(text, chunk_size)\n    # Summarize each chunk\n    chunk_summaries = []\n    for i, chunk in enumerate(chunks):\n        print(f\"Summarizing chunk {i+1}/{len(chunks)}...\")\n        chunk_summary = summarize_with_gpt(model, chunk)\n        chunk_summaries.append(chunk_summary)",
        "detail": "10_summarization_eval.two_step_partial_pass",
        "documentation": {}
    },
    {
        "label": "evaluate_summaries_harimplus",
        "kind": 2,
        "importPath": "10_summarization_eval.two_step_partial_pass",
        "description": "10_summarization_eval.two_step_partial_pass",
        "peekOfCode": "def evaluate_summaries_harimplus(articles, summaries):\n    scorer = evaluate.load(\"NCSOFT/harim_plus\")\n    scores = scorer.compute(predictions=summaries, references=articles)\n    return scores\n# Main script\ndef main():\n    folder_path = \"processed_docs\"  # Folder containing the .txt files\n    output_path = \"summarization_results.json\"  # JSON file to save results\n    # Read the documents\n    documents = read_txt_files(folder_path)",
        "detail": "10_summarization_eval.two_step_partial_pass",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "10_summarization_eval.two_step_partial_pass",
        "description": "10_summarization_eval.two_step_partial_pass",
        "peekOfCode": "def main():\n    folder_path = \"processed_docs\"  # Folder containing the .txt files\n    output_path = \"summarization_results.json\"  # JSON file to save results\n    # Read the documents\n    documents = read_txt_files(folder_path)\n    if not documents:\n        print(\"No documents found in the specified folder.\")\n        return\n    # Prepare models\n    gpt_models = [\"gpt-4o-mini\"]",
        "detail": "10_summarization_eval.two_step_partial_pass",
        "documentation": {}
    },
    {
        "label": "openai.api_key",
        "kind": 5,
        "importPath": "10_summarization_eval.two_step_partial_pass",
        "description": "10_summarization_eval.two_step_partial_pass",
        "peekOfCode": "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n# Generic LLM call function\ndef call_llm(system_prompt, user_prompt, model=\"gpt-4o\", temperature=0.7):\n    \"\"\"Generic LLM call.\"\"\"\n    response = openai.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],",
        "detail": "10_summarization_eval.two_step_partial_pass",
        "documentation": {}
    },
    {
        "label": "load_saved_cursors",
        "kind": 2,
        "importPath": "1_data_collection.index",
        "description": "1_data_collection.index",
        "peekOfCode": "def load_saved_cursors(file_path=\"cursors.json\"):\n    if os.path.exists(file_path):\n        with open(file_path, \"r\") as file:\n            return json.load(file)\n    return {}\n# Save cursors to a JSON file\ndef save_cursors(cursors, file_path=\"cursors.json\"):\n    with open(file_path, \"w\") as file:\n        json.dump(cursors, file, indent=4)\ndef fetch_pr_data(start_file_index=0):",
        "detail": "1_data_collection.index",
        "documentation": {}
    },
    {
        "label": "save_cursors",
        "kind": 2,
        "importPath": "1_data_collection.index",
        "description": "1_data_collection.index",
        "peekOfCode": "def save_cursors(cursors, file_path=\"cursors.json\"):\n    with open(file_path, \"w\") as file:\n        json.dump(cursors, file, indent=4)\ndef fetch_pr_data(start_file_index=0):\n    # Load saved cursors\n    cursors = load_saved_cursors()\n    rate_limit_retry_time = 60  # Retry after 1 minute on rate limit\n    first = 50  # Number of PRs to fetch per query\n    file_index = start_file_index\n    after_cursor = cursors.get(str(file_index), None)",
        "detail": "1_data_collection.index",
        "documentation": {}
    },
    {
        "label": "fetch_pr_data",
        "kind": 2,
        "importPath": "1_data_collection.index",
        "description": "1_data_collection.index",
        "peekOfCode": "def fetch_pr_data(start_file_index=0):\n    # Load saved cursors\n    cursors = load_saved_cursors()\n    rate_limit_retry_time = 60  # Retry after 1 minute on rate limit\n    first = 50  # Number of PRs to fetch per query\n    file_index = start_file_index\n    after_cursor = cursors.get(str(file_index), None)\n    # Create folder for storing JSON files\n    output_folder = \"pr_data\"\n    os.makedirs(output_folder, exist_ok=True)",
        "detail": "1_data_collection.index",
        "documentation": {}
    },
    {
        "label": "GITHUB_TOKEN",
        "kind": 5,
        "importPath": "1_data_collection.index",
        "description": "1_data_collection.index",
        "peekOfCode": "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\nREPO_OWNER = os.getenv(\"REPO_OWNER\")\nREPO_NAME = os.getenv(\"REPO_NAME\")\n# GitHub GraphQL endpoint\nGRAPHQL_URL = \"https://api.github.com/graphql\"\nGRAPHQL_HEADERS = {\n    \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n    \"Content-Type\": \"application/json\"\n}\n# REST API headers",
        "detail": "1_data_collection.index",
        "documentation": {}
    },
    {
        "label": "REPO_OWNER",
        "kind": 5,
        "importPath": "1_data_collection.index",
        "description": "1_data_collection.index",
        "peekOfCode": "REPO_OWNER = os.getenv(\"REPO_OWNER\")\nREPO_NAME = os.getenv(\"REPO_NAME\")\n# GitHub GraphQL endpoint\nGRAPHQL_URL = \"https://api.github.com/graphql\"\nGRAPHQL_HEADERS = {\n    \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n    \"Content-Type\": \"application/json\"\n}\n# REST API headers\nREST_HEADERS = {",
        "detail": "1_data_collection.index",
        "documentation": {}
    },
    {
        "label": "REPO_NAME",
        "kind": 5,
        "importPath": "1_data_collection.index",
        "description": "1_data_collection.index",
        "peekOfCode": "REPO_NAME = os.getenv(\"REPO_NAME\")\n# GitHub GraphQL endpoint\nGRAPHQL_URL = \"https://api.github.com/graphql\"\nGRAPHQL_HEADERS = {\n    \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n    \"Content-Type\": \"application/json\"\n}\n# REST API headers\nREST_HEADERS = {\n    \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",",
        "detail": "1_data_collection.index",
        "documentation": {}
    },
    {
        "label": "GRAPHQL_URL",
        "kind": 5,
        "importPath": "1_data_collection.index",
        "description": "1_data_collection.index",
        "peekOfCode": "GRAPHQL_URL = \"https://api.github.com/graphql\"\nGRAPHQL_HEADERS = {\n    \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n    \"Content-Type\": \"application/json\"\n}\n# REST API headers\nREST_HEADERS = {\n    \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n    \"Accept\": \"application/vnd.github.v3+json\"\n}",
        "detail": "1_data_collection.index",
        "documentation": {}
    },
    {
        "label": "GRAPHQL_HEADERS",
        "kind": 5,
        "importPath": "1_data_collection.index",
        "description": "1_data_collection.index",
        "peekOfCode": "GRAPHQL_HEADERS = {\n    \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n    \"Content-Type\": \"application/json\"\n}\n# REST API headers\nREST_HEADERS = {\n    \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n    \"Accept\": \"application/vnd.github.v3+json\"\n}\n# GraphQL query with pagination",
        "detail": "1_data_collection.index",
        "documentation": {}
    },
    {
        "label": "REST_HEADERS",
        "kind": 5,
        "importPath": "1_data_collection.index",
        "description": "1_data_collection.index",
        "peekOfCode": "REST_HEADERS = {\n    \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n    \"Accept\": \"application/vnd.github.v3+json\"\n}\n# GraphQL query with pagination\nQUERY = \"\"\"\nquery ($owner: String!, $name: String!, $first: Int!, $after: String) {\n  repository(owner: $owner, name: $name) {\n    pullRequests(first: $first, after: $after, states: [OPEN, MERGED, CLOSED], orderBy: {field: CREATED_AT, direction: DESC}) {\n      nodes {",
        "detail": "1_data_collection.index",
        "documentation": {}
    },
    {
        "label": "QUERY",
        "kind": 5,
        "importPath": "1_data_collection.index",
        "description": "1_data_collection.index",
        "peekOfCode": "QUERY = \"\"\"\nquery ($owner: String!, $name: String!, $first: Int!, $after: String) {\n  repository(owner: $owner, name: $name) {\n    pullRequests(first: $first, after: $after, states: [OPEN, MERGED, CLOSED], orderBy: {field: CREATED_AT, direction: DESC}) {\n      nodes {\n        number\n        title\n        bodyText\n        baseRefName\n        headRefName",
        "detail": "1_data_collection.index",
        "documentation": {}
    },
    {
        "label": "process_pull_request",
        "kind": 2,
        "importPath": "2_data_cleaning.convert_json_to_txt",
        "description": "2_data_cleaning.convert_json_to_txt",
        "peekOfCode": "def process_pull_request(pr: Dict) -> str:\n    \"\"\"\n    Convert a single pull request dictionary into a document string.\n    \"\"\"\n    files_changed = pr.get('files_with_diffs', [])\n    if not files_changed:\n        files_changed_text = \"No files changed.\"\n    else:\n        files_changed_text = ''.join([\n            (",
        "detail": "2_data_cleaning.convert_json_to_txt",
        "documentation": {}
    },
    {
        "label": "process_json_file",
        "kind": 2,
        "importPath": "2_data_cleaning.convert_json_to_txt",
        "description": "2_data_cleaning.convert_json_to_txt",
        "peekOfCode": "def process_json_file(file_path: str) -> List[str]:\n    \"\"\"\n    Read a JSON file and convert its pull request data to documents.\n    \"\"\"\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n        pr_nodes = data.get('data', {}).get('repository', {}).get(\n            'pullRequests', {}).get('nodes', [])\n        return [process_pull_request(pr) for pr in pr_nodes]\ndef traverse_folder_and_convert_to_documents(folder_path: str, output_path: str):",
        "detail": "2_data_cleaning.convert_json_to_txt",
        "documentation": {}
    },
    {
        "label": "traverse_folder_and_convert_to_documents",
        "kind": 2,
        "importPath": "2_data_cleaning.convert_json_to_txt",
        "description": "2_data_cleaning.convert_json_to_txt",
        "peekOfCode": "def traverse_folder_and_convert_to_documents(folder_path: str, output_path: str):\n    \"\"\"\n    Traverse the folder, process JSON files in order, and save documents to the output folder.\n    \"\"\"\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n    # Sort files in the order of `pr_data_0.json` to `pr_data_207.json`\n    files = sorted([f for f in os.listdir(folder_path) if f.endswith('.json')],\n                   key=lambda x: int(x.split('_')[-1].split('.')[0]))\n    for file in files:",
        "detail": "2_data_cleaning.convert_json_to_txt",
        "documentation": {}
    },
    {
        "label": "RelevanceScore",
        "kind": 6,
        "importPath": "3_retriever.1_gte_small_githubv2.eval",
        "description": "3_retriever.1_gte_small_githubv2.eval",
        "peekOfCode": "class RelevanceScore(BaseModel):\n    score: int = Field(\n        description=\"The relevance score of the retrieval from 0 to 10\")\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. ",
        "detail": "3_retriever.1_gte_small_githubv2.eval",
        "documentation": {}
    },
    {
        "label": "query_llm",
        "kind": 2,
        "importPath": "3_retriever.1_gte_small_githubv2.eval",
        "description": "3_retriever.1_gte_small_githubv2.eval",
        "peekOfCode": "def query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:\n    - Assess how specifically and accurately the PR addresses or provides relevant information for the query.\n    - Consider whether the PR contains meaningful file changes, code implementations, or documentation that directly answer the query.",
        "detail": "3_retriever.1_gte_small_githubv2.eval",
        "documentation": {}
    },
    {
        "label": "questions",
        "kind": 5,
        "importPath": "3_retriever.1_gte_small_githubv2.eval",
        "description": "3_retriever.1_gte_small_githubv2.eval",
        "peekOfCode": "questions = [[\n    \"pr_data_0_doc_7.txt\",\n    \"How can I enhance the validation capabilities of a Select component to prevent the default browser error UI from appearing, while also supporting required fields, custom validation functions, and server-side validation? Additionally, what changes would be needed to integrate these features into the multi-select component for better user interaction and validation logic?\"\n],\n    [\n        \"pr_data_21_doc_3.txt\",\n        \"How can I extend the props for the NextUIProvider to allow direct styling of the container element created by the OverlayProvider, and ensure that the children components are correctly inheriting styles from the body element? Additionally, how can I update the default locale setting within the NextUIProvider without introducing breaking changes?\"\n],\n    [\n        \"pr_data_27_doc_4.txt\",",
        "detail": "3_retriever.1_gte_small_githubv2.eval",
        "documentation": {}
    },
    {
        "label": "VECTOR_DB_DIR",
        "kind": 5,
        "importPath": "3_retriever.1_gte_small_githubv2.eval",
        "description": "3_retriever.1_gte_small_githubv2.eval",
        "peekOfCode": "VECTOR_DB_DIR = \"vector_db\"\nMODEL_NAME = \"thenlper/gte-small\"\n# Use OpenAI embeddings if applicable\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    # Ensure the device matches the previous setup\n    model_kwargs={\"device\": \"mps\"},\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(",
        "detail": "3_retriever.1_gte_small_githubv2.eval",
        "documentation": {}
    },
    {
        "label": "MODEL_NAME",
        "kind": 5,
        "importPath": "3_retriever.1_gte_small_githubv2.eval",
        "description": "3_retriever.1_gte_small_githubv2.eval",
        "peekOfCode": "MODEL_NAME = \"thenlper/gte-small\"\n# Use OpenAI embeddings if applicable\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    # Ensure the device matches the previous setup\n    model_kwargs={\"device\": \"mps\"},\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,",
        "detail": "3_retriever.1_gte_small_githubv2.eval",
        "documentation": {}
    },
    {
        "label": "embedding_model",
        "kind": 5,
        "importPath": "3_retriever.1_gte_small_githubv2.eval",
        "description": "3_retriever.1_gte_small_githubv2.eval",
        "peekOfCode": "embedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    # Ensure the device matches the previous setup\n    model_kwargs={\"device\": \"mps\"},\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,\n    embedding_function=embedding_model\n)",
        "detail": "3_retriever.1_gte_small_githubv2.eval",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "3_retriever.1_gte_small_githubv2.eval",
        "description": "3_retriever.1_gte_small_githubv2.eval",
        "peekOfCode": "vectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,\n    embedding_function=embedding_model\n)\nretriever = vectorstore.as_retriever(\n    search_type=\"similarity\", search_kwargs={\"k\": 5})\n# Step 3: Initialize the LLM judge using GPT-4\nclass RelevanceScore(BaseModel):\n    score: int = Field(\n        description=\"The relevance score of the retrieval from 0 to 10\")",
        "detail": "3_retriever.1_gte_small_githubv2.eval",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "3_retriever.1_gte_small_githubv2.eval",
        "description": "3_retriever.1_gte_small_githubv2.eval",
        "peekOfCode": "retriever = vectorstore.as_retriever(\n    search_type=\"similarity\", search_kwargs={\"k\": 5})\n# Step 3: Initialize the LLM judge using GPT-4\nclass RelevanceScore(BaseModel):\n    score: int = Field(\n        description=\"The relevance score of the retrieval from 0 to 10\")\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"",
        "detail": "3_retriever.1_gte_small_githubv2.eval",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "3_retriever.1_gte_small_githubv2.eval",
        "description": "3_retriever.1_gte_small_githubv2.eval",
        "peekOfCode": "model = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:",
        "detail": "3_retriever.1_gte_small_githubv2.eval",
        "documentation": {}
    },
    {
        "label": "structured_llm",
        "kind": 5,
        "importPath": "3_retriever.1_gte_small_githubv2.eval",
        "description": "3_retriever.1_gte_small_githubv2.eval",
        "peekOfCode": "structured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:\n    - Assess how specifically and accurately the PR addresses or provides relevant information for the query.",
        "detail": "3_retriever.1_gte_small_githubv2.eval",
        "documentation": {}
    },
    {
        "label": "RELEVANCE_THRESHOLD",
        "kind": 5,
        "importPath": "3_retriever.1_gte_small_githubv2.eval",
        "description": "3_retriever.1_gte_small_githubv2.eval",
        "peekOfCode": "RELEVANCE_THRESHOLD = 7\n# Step 4: Evaluate each question\nresults = {}\noverall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []",
        "detail": "3_retriever.1_gte_small_githubv2.eval",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "3_retriever.1_gte_small_githubv2.eval",
        "description": "3_retriever.1_gte_small_githubv2.eval",
        "peekOfCode": "results = {}\noverall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []\n}\nfor [file_name_query, question] in questions:",
        "detail": "3_retriever.1_gte_small_githubv2.eval",
        "documentation": {}
    },
    {
        "label": "overall_metrics",
        "kind": 5,
        "importPath": "3_retriever.1_gte_small_githubv2.eval",
        "description": "3_retriever.1_gte_small_githubv2.eval",
        "peekOfCode": "overall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []\n}\nfor [file_name_query, question] in questions:\n    print(\"Evaluating: \", question)",
        "detail": "3_retriever.1_gte_small_githubv2.eval",
        "documentation": {}
    },
    {
        "label": "results[\"overall_averages\"]",
        "kind": 5,
        "importPath": "3_retriever.1_gte_small_githubv2.eval",
        "description": "3_retriever.1_gte_small_githubv2.eval",
        "peekOfCode": "results[\"overall_averages\"] = {\n    metric: np.mean(values) for metric, values in overall_metrics.items()\n}\n# Save results to JSON files\nwith open(\"evaluation_results_v2.json\", \"w\") as json_file:\n    json.dump(results, json_file, indent=4)\nprint(\"Results have been saved to evaluation_results.json.\")\nprint(\"Overall metrics have been saved to overall_metrics.json.\")",
        "detail": "3_retriever.1_gte_small_githubv2.eval",
        "documentation": {}
    },
    {
        "label": "generate_random_samples",
        "kind": 2,
        "importPath": "4_bm25._temp_generate_pr_questions",
        "description": "4_bm25._temp_generate_pr_questions",
        "peekOfCode": "def generate_random_samples():\n    \"\"\"Generate NUM_SAMPLES random samples in the format pr_data_{x}_doc_{y}.txt.\"\"\"\n    samples = []\n    for _ in range(NUM_SAMPLES):\n        x = random.randint(0, 32)  # Range for 'pr_data_x'\n        y = random.randint(1, 49)  # Range for 'doc_y'\n        samples.append(f\"pr_data_{x}_doc_{y}.txt\")\n    return samples\ndef read_pr_data(file_path):\n    \"\"\"Reads content from the PR file.\"\"\"",
        "detail": "4_bm25._temp_generate_pr_questions",
        "documentation": {}
    },
    {
        "label": "read_pr_data",
        "kind": 2,
        "importPath": "4_bm25._temp_generate_pr_questions",
        "description": "4_bm25._temp_generate_pr_questions",
        "peekOfCode": "def read_pr_data(file_path):\n    \"\"\"Reads content from the PR file.\"\"\"\n    try:\n        with open(file_path, \"r\") as file:\n            return file.read()\n    except FileNotFoundError:\n        return None\ndef generate_question(pr_content):\n    \"\"\"Generates a developer-style question using ChatGPT API.\"\"\"\n    prompt = f\"\"\"",
        "detail": "4_bm25._temp_generate_pr_questions",
        "documentation": {}
    },
    {
        "label": "generate_question",
        "kind": 2,
        "importPath": "4_bm25._temp_generate_pr_questions",
        "description": "4_bm25._temp_generate_pr_questions",
        "peekOfCode": "def generate_question(pr_content):\n    \"\"\"Generates a developer-style question using ChatGPT API.\"\"\"\n    prompt = f\"\"\"\n    Imagine you are a developer trying to understand or learn something specific for your work.\n    Your boss suggests looking at the following Pull Request to understand it better:\n    {pr_content}\n    Your question should not be asking about what are the code changes or what is the PR about.\n    Your question is more like -> you are given a ticket from PM (feature or bug or refactor) -> assume one of this -> what would you ask if you want to start coding.\n    Normally a developer will be start querying the old PRs to see how others have done it or not.\n    Write the exact question you would ask that, if answered, the answer would directly be the content of this PR.",
        "detail": "4_bm25._temp_generate_pr_questions",
        "documentation": {}
    },
    {
        "label": "process_samples",
        "kind": 2,
        "importPath": "4_bm25._temp_generate_pr_questions",
        "description": "4_bm25._temp_generate_pr_questions",
        "peekOfCode": "def process_samples(samples, base_path):\n    \"\"\"Processes the sample PR files and generates developer questions.\"\"\"\n    results = []\n    i = 0\n    for sample in samples:\n        print(\"Geneerating\", i)\n        file_path = os.path.join(base_path, sample)\n        pr_content = read_pr_data(file_path)\n        if pr_content:\n            question = generate_question(pr_content)",
        "detail": "4_bm25._temp_generate_pr_questions",
        "documentation": {}
    },
    {
        "label": "OPENAI_API_KEY",
        "kind": 5,
        "importPath": "4_bm25._temp_generate_pr_questions",
        "description": "4_bm25._temp_generate_pr_questions",
        "peekOfCode": "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n# Update with the directory containing PR files\nBASE_PATH = \"processed_docs\"\nRESULT_JSON_FILE = \"questions_for_PR.json\"     # Output file for storing results\nNUM_SAMPLES = 10                     # Number of random samples to generate\n# Set OpenAI API key\nclient = openai.OpenAI()\ndef generate_random_samples():\n    \"\"\"Generate NUM_SAMPLES random samples in the format pr_data_{x}_doc_{y}.txt.\"\"\"\n    samples = []",
        "detail": "4_bm25._temp_generate_pr_questions",
        "documentation": {}
    },
    {
        "label": "BASE_PATH",
        "kind": 5,
        "importPath": "4_bm25._temp_generate_pr_questions",
        "description": "4_bm25._temp_generate_pr_questions",
        "peekOfCode": "BASE_PATH = \"processed_docs\"\nRESULT_JSON_FILE = \"questions_for_PR.json\"     # Output file for storing results\nNUM_SAMPLES = 10                     # Number of random samples to generate\n# Set OpenAI API key\nclient = openai.OpenAI()\ndef generate_random_samples():\n    \"\"\"Generate NUM_SAMPLES random samples in the format pr_data_{x}_doc_{y}.txt.\"\"\"\n    samples = []\n    for _ in range(NUM_SAMPLES):\n        x = random.randint(0, 32)  # Range for 'pr_data_x'",
        "detail": "4_bm25._temp_generate_pr_questions",
        "documentation": {}
    },
    {
        "label": "RESULT_JSON_FILE",
        "kind": 5,
        "importPath": "4_bm25._temp_generate_pr_questions",
        "description": "4_bm25._temp_generate_pr_questions",
        "peekOfCode": "RESULT_JSON_FILE = \"questions_for_PR.json\"     # Output file for storing results\nNUM_SAMPLES = 10                     # Number of random samples to generate\n# Set OpenAI API key\nclient = openai.OpenAI()\ndef generate_random_samples():\n    \"\"\"Generate NUM_SAMPLES random samples in the format pr_data_{x}_doc_{y}.txt.\"\"\"\n    samples = []\n    for _ in range(NUM_SAMPLES):\n        x = random.randint(0, 32)  # Range for 'pr_data_x'\n        y = random.randint(1, 49)  # Range for 'doc_y'",
        "detail": "4_bm25._temp_generate_pr_questions",
        "documentation": {}
    },
    {
        "label": "NUM_SAMPLES",
        "kind": 5,
        "importPath": "4_bm25._temp_generate_pr_questions",
        "description": "4_bm25._temp_generate_pr_questions",
        "peekOfCode": "NUM_SAMPLES = 10                     # Number of random samples to generate\n# Set OpenAI API key\nclient = openai.OpenAI()\ndef generate_random_samples():\n    \"\"\"Generate NUM_SAMPLES random samples in the format pr_data_{x}_doc_{y}.txt.\"\"\"\n    samples = []\n    for _ in range(NUM_SAMPLES):\n        x = random.randint(0, 32)  # Range for 'pr_data_x'\n        y = random.randint(1, 49)  # Range for 'doc_y'\n        samples.append(f\"pr_data_{x}_doc_{y}.txt\")",
        "detail": "4_bm25._temp_generate_pr_questions",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "4_bm25._temp_generate_pr_questions",
        "description": "4_bm25._temp_generate_pr_questions",
        "peekOfCode": "client = openai.OpenAI()\ndef generate_random_samples():\n    \"\"\"Generate NUM_SAMPLES random samples in the format pr_data_{x}_doc_{y}.txt.\"\"\"\n    samples = []\n    for _ in range(NUM_SAMPLES):\n        x = random.randint(0, 32)  # Range for 'pr_data_x'\n        y = random.randint(1, 49)  # Range for 'doc_y'\n        samples.append(f\"pr_data_{x}_doc_{y}.txt\")\n    return samples\ndef read_pr_data(file_path):",
        "detail": "4_bm25._temp_generate_pr_questions",
        "documentation": {}
    },
    {
        "label": "RelevanceScore",
        "kind": 6,
        "importPath": "4_bm25.eval",
        "description": "4_bm25.eval",
        "peekOfCode": "class RelevanceScore(BaseModel):\n    score: int = Field(\n        description=\"The relevance score of the retrieval from 0 to 10\")\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. ",
        "detail": "4_bm25.eval",
        "documentation": {}
    },
    {
        "label": "query_llm",
        "kind": 2,
        "importPath": "4_bm25.eval",
        "description": "4_bm25.eval",
        "peekOfCode": "def query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:\n    - Assess how specifically and accurately the PR addresses or provides relevant information for the query.\n    - Consider whether the PR contains meaningful file changes, code implementations, or documentation that directly answer the query.",
        "detail": "4_bm25.eval",
        "documentation": {}
    },
    {
        "label": "questions",
        "kind": 5,
        "importPath": "4_bm25.eval",
        "description": "4_bm25.eval",
        "peekOfCode": "questions = [[\n    \"pr_data_0_doc_7.txt\",\n    \"How can I enhance the validation capabilities of a Select component to prevent the default browser error UI from appearing, while also supporting required fields, custom validation functions, and server-side validation? Additionally, what changes would be needed to integrate these features into the multi-select component for better user interaction and validation logic?\"\n],\n    [\n        \"pr_data_21_doc_3.txt\",\n        \"How can I extend the props for the NextUIProvider to allow direct styling of the container element created by the OverlayProvider, and ensure that the children components are correctly inheriting styles from the body element? Additionally, how can I update the default locale setting within the NextUIProvider without introducing breaking changes?\"\n],\n    [\n        \"pr_data_27_doc_4.txt\",",
        "detail": "4_bm25.eval",
        "documentation": {}
    },
    {
        "label": "folder_path",
        "kind": 5,
        "importPath": "4_bm25.eval",
        "description": "4_bm25.eval",
        "peekOfCode": "folder_path = \"processed_docs\"  # Ensure the path matches your folder structure\ndocuments = _read_documents_from_folder(folder_path)\nretriever = BM25Retriever(documents)\n# Step 3: Initialize the LLM judge using GPT-4\nclass RelevanceScore(BaseModel):\n    score: int = Field(\n        description=\"The relevance score of the retrieval from 0 to 10\")\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):",
        "detail": "4_bm25.eval",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "4_bm25.eval",
        "description": "4_bm25.eval",
        "peekOfCode": "documents = _read_documents_from_folder(folder_path)\nretriever = BM25Retriever(documents)\n# Step 3: Initialize the LLM judge using GPT-4\nclass RelevanceScore(BaseModel):\n    score: int = Field(\n        description=\"The relevance score of the retrieval from 0 to 10\")\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"",
        "detail": "4_bm25.eval",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "4_bm25.eval",
        "description": "4_bm25.eval",
        "peekOfCode": "retriever = BM25Retriever(documents)\n# Step 3: Initialize the LLM judge using GPT-4\nclass RelevanceScore(BaseModel):\n    score: int = Field(\n        description=\"The relevance score of the retrieval from 0 to 10\")\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"",
        "detail": "4_bm25.eval",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "4_bm25.eval",
        "description": "4_bm25.eval",
        "peekOfCode": "model = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:",
        "detail": "4_bm25.eval",
        "documentation": {}
    },
    {
        "label": "structured_llm",
        "kind": 5,
        "importPath": "4_bm25.eval",
        "description": "4_bm25.eval",
        "peekOfCode": "structured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:\n    - Assess how specifically and accurately the PR addresses or provides relevant information for the query.",
        "detail": "4_bm25.eval",
        "documentation": {}
    },
    {
        "label": "RELEVANCE_THRESHOLD",
        "kind": 5,
        "importPath": "4_bm25.eval",
        "description": "4_bm25.eval",
        "peekOfCode": "RELEVANCE_THRESHOLD = 7\n# Step 4: Evaluate each question\nresults = {}\noverall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []",
        "detail": "4_bm25.eval",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "4_bm25.eval",
        "description": "4_bm25.eval",
        "peekOfCode": "results = {}\noverall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []\n}\nfor [file_name_query, question] in questions:",
        "detail": "4_bm25.eval",
        "documentation": {}
    },
    {
        "label": "overall_metrics",
        "kind": 5,
        "importPath": "4_bm25.eval",
        "description": "4_bm25.eval",
        "peekOfCode": "overall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []\n}\nfor [file_name_query, question] in questions:\n    print(\"Evaluating: \", question)",
        "detail": "4_bm25.eval",
        "documentation": {}
    },
    {
        "label": "results[\"overall_averages\"]",
        "kind": 5,
        "importPath": "4_bm25.eval",
        "description": "4_bm25.eval",
        "peekOfCode": "results[\"overall_averages\"] = {\n    metric: np.mean(values) for metric, values in overall_metrics.items()\n}\n# Save results to JSON files\nwith open(\"evaluation_results_v2.json\", \"w\") as json_file:\n    json.dump(results, json_file, indent=4)\nprint(\"Results have been saved to evaluation_results.json.\")\nprint(\"Overall metrics have been saved to overall_metrics.json.\")",
        "detail": "4_bm25.eval",
        "documentation": {}
    },
    {
        "label": "BM25Retriever",
        "kind": 6,
        "importPath": "4_bm25.index",
        "description": "4_bm25.index",
        "peekOfCode": "class BM25Retriever:\n    def __init__(self, documents):\n        \"\"\"\n        Initialize BM25 Retriever using rank-bm25 library.\n        :param documents: List of documents (each document is a string).\n        \"\"\"\n        self.tokenized_documents = [doc.split()\n                                    for [file_name, doc] in documents]\n        self.bm25 = BM25Okapi(self.tokenized_documents)\n    def retrieve(self, query, top_n=5):",
        "detail": "4_bm25.index",
        "documentation": {}
    },
    {
        "label": "BM25Retriever",
        "kind": 6,
        "importPath": "5_ensemble.bm25",
        "description": "5_ensemble.bm25",
        "peekOfCode": "class BM25Retriever:\n    def __init__(self, documents):\n        \"\"\"\n        Initialize BM25 Retriever using rank-bm25 library.\n        :param documents: List of documents (each document is a string).\n        \"\"\"\n        self.tokenized_documents = [doc.split()\n                                    for [file_name, doc] in documents]\n        self.bm25 = BM25Okapi(self.tokenized_documents)\n    def retrieve(self, query, top_n=5):",
        "detail": "5_ensemble.bm25",
        "documentation": {}
    },
    {
        "label": "RelevanceScore",
        "kind": 6,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "class RelevanceScore(BaseModel):\n    score: int = Field(\n        description=\"The relevance score of the retrieval from 0 to 10\")\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. ",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "query_llm",
        "kind": 2,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "def query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:\n    - Assess how specifically and accurately the PR addresses or provides relevant information for the query.\n    - Consider whether the PR contains meaningful file changes, code implementations, or documentation that directly answer the query.",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "questions",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "questions = [[\n    \"pr_data_0_doc_7.txt\",\n    \"How can I enhance the validation capabilities of a Select component to prevent the default browser error UI from appearing, while also supporting required fields, custom validation functions, and server-side validation? Additionally, what changes would be needed to integrate these features into the multi-select component for better user interaction and validation logic?\"\n],\n    [\n        \"pr_data_21_doc_3.txt\",\n        \"How can I extend the props for the NextUIProvider to allow direct styling of the container element created by the OverlayProvider, and ensure that the children components are correctly inheriting styles from the body element? Additionally, how can I update the default locale setting within the NextUIProvider without introducing breaking changes?\"\n],\n    [\n        \"pr_data_27_doc_4.txt\",",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "folder_path",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "folder_path = \"processed_docs\"  # Ensure the path matches your folder structure\ndocuments = _read_documents_from_folder(folder_path)\n# Create a mapping from file_name to content for quick lookup\ndoc_dict = {doc[0]: doc[1] for doc in documents}\n# Initialize BM25 Retriever\nbm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"vector_db\"\nMODEL_NAME = \"thenlper/gte-small\"\nembedding_model = HuggingFaceEmbeddings(",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "documents = _read_documents_from_folder(folder_path)\n# Create a mapping from file_name to content for quick lookup\ndoc_dict = {doc[0]: doc[1] for doc in documents}\n# Initialize BM25 Retriever\nbm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"vector_db\"\nMODEL_NAME = \"thenlper/gte-small\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "doc_dict",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "doc_dict = {doc[0]: doc[1] for doc in documents}\n# Initialize BM25 Retriever\nbm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"vector_db\"\nMODEL_NAME = \"thenlper/gte-small\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # or \"cpu\"/\"cuda\" depending on your system\n    encode_kwargs={\"normalize_embeddings\": True}",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "bm25_retriever",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "bm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"vector_db\"\nMODEL_NAME = \"thenlper/gte-small\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # or \"cpu\"/\"cuda\" depending on your system\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "VECTOR_DB_DIR",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "VECTOR_DB_DIR = \"vector_db\"\nMODEL_NAME = \"thenlper/gte-small\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # or \"cpu\"/\"cuda\" depending on your system\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,\n    embedding_function=embedding_model",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "MODEL_NAME",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "MODEL_NAME = \"thenlper/gte-small\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # or \"cpu\"/\"cuda\" depending on your system\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,\n    embedding_function=embedding_model\n)",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "embedding_model",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "embedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # or \"cpu\"/\"cuda\" depending on your system\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,\n    embedding_function=embedding_model\n)\nvector_retriever = vectorstore.as_retriever(",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "vectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,\n    embedding_function=embedding_model\n)\nvector_retriever = vectorstore.as_retriever(\n    search_type=\"similarity\", search_kwargs={\"k\": 5})\n# Step 3: Initialize the LLM judge using GPT-4\nclass RelevanceScore(BaseModel):\n    score: int = Field(\n        description=\"The relevance score of the retrieval from 0 to 10\")",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "vector_retriever",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "vector_retriever = vectorstore.as_retriever(\n    search_type=\"similarity\", search_kwargs={\"k\": 5})\n# Step 3: Initialize the LLM judge using GPT-4\nclass RelevanceScore(BaseModel):\n    score: int = Field(\n        description=\"The relevance score of the retrieval from 0 to 10\")\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "model = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "structured_llm",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "structured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:\n    - Assess how specifically and accurately the PR addresses or provides relevant information for the query.",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "RELEVANCE_THRESHOLD",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "RELEVANCE_THRESHOLD = 7\n# Step 4: Evaluate each question\nresults = {}\noverall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "results = {}\noverall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []\n}\nfor [file_name_query, question] in questions:",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "overall_metrics",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "overall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []\n}\nfor [file_name_query, question] in questions:\n    print(\"Evaluating: \", question)",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "results[\"overall_averages\"]",
        "kind": 5,
        "importPath": "5_ensemble.index",
        "description": "5_ensemble.index",
        "peekOfCode": "results[\"overall_averages\"] = {\n    metric: np.mean(values) for metric, values in overall_metrics.items()\n}\n# Save results to JSON file\nwith open(\"evaluation_results_ensemble.json\", \"w\") as json_file:\n    json.dump(results, json_file, indent=4)\nprint(\"Results have been saved to evaluation_results_ensemble.json.\")",
        "detail": "5_ensemble.index",
        "documentation": {}
    },
    {
        "label": "BM25Retriever",
        "kind": 6,
        "importPath": "6_reranking.bm25",
        "description": "6_reranking.bm25",
        "peekOfCode": "class BM25Retriever:\n    def __init__(self, documents):\n        \"\"\"\n        Initialize BM25 Retriever using rank-bm25 library.\n        :param documents: List of documents (each document is a string).\n        \"\"\"\n        self.tokenized_documents = [doc.split()\n                                    for [file_name, doc] in documents]\n        self.bm25 = BM25Okapi(self.tokenized_documents)\n    def retrieve(self, query, top_n=5):",
        "detail": "6_reranking.bm25",
        "documentation": {}
    },
    {
        "label": "RelevanceScore",
        "kind": 6,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "class RelevanceScore(BaseModel):\n    score: int = Field(\n        description=\"The relevance score of the retrieval from 0 to 10\")\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. ",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "query_llm",
        "kind": 2,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "def query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:\n    - Assess how specifically and accurately the PR addresses or provides relevant information for the query.\n    - Consider whether the PR contains meaningful file changes, code implementations, or documentation that directly answer the query.",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "questions",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "questions = [[\n    \"pr_data_0_doc_7.txt\",\n    \"How can I enhance the validation capabilities of a Select component to prevent the default browser error UI from appearing, while also supporting required fields, custom validation functions, and server-side validation? Additionally, what changes would be needed to integrate these features into the multi-select component for better user interaction and validation logic?\"\n],\n    [\n        \"pr_data_21_doc_3.txt\",\n        \"How can I extend the props for the NextUIProvider to allow direct styling of the container element created by the OverlayProvider, and ensure that the children components are correctly inheriting styles from the body element? Additionally, how can I update the default locale setting within the NextUIProvider without introducing breaking changes?\"\n],\n    [\n        \"pr_data_27_doc_4.txt\",",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "top_n_initial",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "top_n_initial = 15  # Get initial top 10 or 15, configurable\ntop_n_final = 5     # After reranking, choose top 5\n# Step 2: Load your documents\nfolder_path = \"processed_docs\"  # Ensure the path matches your folder structure\ndocuments = _read_documents_from_folder(folder_path)\ndoc_dict = {doc[0]: doc[1] for doc in documents}\n# Initialize BM25 Retriever\nbm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"final_all-MiniLM-L6-v2\"",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "top_n_final",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "top_n_final = 5     # After reranking, choose top 5\n# Step 2: Load your documents\nfolder_path = \"processed_docs\"  # Ensure the path matches your folder structure\ndocuments = _read_documents_from_folder(folder_path)\ndoc_dict = {doc[0]: doc[1] for doc in documents}\n# Initialize BM25 Retriever\nbm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"final_all-MiniLM-L6-v2\"\nMODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "folder_path",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "folder_path = \"processed_docs\"  # Ensure the path matches your folder structure\ndocuments = _read_documents_from_folder(folder_path)\ndoc_dict = {doc[0]: doc[1] for doc in documents}\n# Initialize BM25 Retriever\nbm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"final_all-MiniLM-L6-v2\"\nMODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "documents = _read_documents_from_folder(folder_path)\ndoc_dict = {doc[0]: doc[1] for doc in documents}\n# Initialize BM25 Retriever\nbm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"final_all-MiniLM-L6-v2\"\nMODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # adjust as needed",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "doc_dict",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "doc_dict = {doc[0]: doc[1] for doc in documents}\n# Initialize BM25 Retriever\nbm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"final_all-MiniLM-L6-v2\"\nMODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # adjust as needed\n    encode_kwargs={\"normalize_embeddings\": True}",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "bm25_retriever",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "bm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"final_all-MiniLM-L6-v2\"\nMODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # adjust as needed\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "VECTOR_DB_DIR",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "VECTOR_DB_DIR = \"final_all-MiniLM-L6-v2\"\nMODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # adjust as needed\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,\n    embedding_function=embedding_model",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "MODEL_NAME",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # adjust as needed\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,\n    embedding_function=embedding_model\n)",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "embedding_model",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "embedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # adjust as needed\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,\n    embedding_function=embedding_model\n)\nvector_retriever = vectorstore.as_retriever(",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "vectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,\n    embedding_function=embedding_model\n)\nvector_retriever = vectorstore.as_retriever(\n    # we will control top_n later\n    search_type=\"similarity\", search_kwargs={\"k\": top_n_initial}\n)\n# Step 3: Initialize the LLM judge using GPT-4 (unchanged)\nclass RelevanceScore(BaseModel):",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "vector_retriever",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "vector_retriever = vectorstore.as_retriever(\n    # we will control top_n later\n    search_type=\"similarity\", search_kwargs={\"k\": top_n_initial}\n)\n# Step 3: Initialize the LLM judge using GPT-4 (unchanged)\nclass RelevanceScore(BaseModel):\n    score: int = Field(\n        description=\"The relevance score of the retrieval from 0 to 10\")\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "model = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "structured_llm",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "structured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:\n    - Assess how specifically and accurately the PR addresses or provides relevant information for the query.",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "cross_encoder_model",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "cross_encoder_model = CrossEncoder(\"BAAI/bge-reranker-base\")\n# Define thresholds for relevance\nRELEVANCE_THRESHOLD = 7\nresults = {}\noverall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "RELEVANCE_THRESHOLD",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "RELEVANCE_THRESHOLD = 7\nresults = {}\noverall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []\n}",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "results = {}\noverall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []\n}\nfor [file_name_query, question] in questions:",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "overall_metrics",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "overall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []\n}\nfor [file_name_query, question] in questions:\n    print(\"Evaluating: \", question)",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "results[\"overall_averages\"]",
        "kind": 5,
        "importPath": "6_reranking.with_reranking",
        "description": "6_reranking.with_reranking",
        "peekOfCode": "results[\"overall_averages\"] = {\n    metric: np.mean(values) for metric, values in overall_metrics.items()\n}\nwith open(\"evaluation_results_ensemble.json\", \"w\") as json_file:\n    json.dump(results, json_file, indent=4)\nprint(\"Results have been saved to evaluation_results_ensemble.json.\")",
        "detail": "6_reranking.with_reranking",
        "documentation": {}
    },
    {
        "label": "RelevanceScore",
        "kind": 6,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "class RelevanceScore(BaseModel):\n    score: int = Field(\n        description=\"The relevance score of the retrieval from 0 to 10\")\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. ",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "query_llm",
        "kind": 2,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "def query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:\n    - Assess how specifically and accurately the PR addresses or provides relevant information for the query.\n    - Consider whether the PR contains meaningful file changes, code implementations, or documentation that directly answer the query.",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "questions",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "questions = [[\n    \"pr_data_0_doc_7.txt\",\n    \"How can I enhance the validation capabilities of a Select component to prevent the default browser error UI from appearing, while also supporting required fields, custom validation functions, and server-side validation? Additionally, what changes would be needed to integrate these features into the multi-select component for better user interaction and validation logic?\"\n],\n    [\n        \"pr_data_21_doc_3.txt\",\n        \"How can I extend the props for the NextUIProvider to allow direct styling of the container element created by the OverlayProvider, and ensure that the children components are correctly inheriting styles from the body element? Additionally, how can I update the default locale setting within the NextUIProvider without introducing breaking changes?\"\n],\n    [\n        \"pr_data_27_doc_4.txt\",",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "top_n_initial",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "top_n_initial = 5  # Get initial top 10 or 15, configurable\ntop_n_final = 5     # After reranking, choose top 5\n# Step 2: Load your documents\nfolder_path = \"processed_docs\"  # Ensure the path matches your folder structure\ndocuments = _read_documents_from_folder(folder_path)\ndoc_dict = {doc[0]: doc[1] for doc in documents}\n# Initialize BM25 Retriever\nbm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"final_all-MiniLM-L6-v2\"",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "top_n_final",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "top_n_final = 5     # After reranking, choose top 5\n# Step 2: Load your documents\nfolder_path = \"processed_docs\"  # Ensure the path matches your folder structure\ndocuments = _read_documents_from_folder(folder_path)\ndoc_dict = {doc[0]: doc[1] for doc in documents}\n# Initialize BM25 Retriever\nbm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"final_all-MiniLM-L6-v2\"\nMODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "folder_path",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "folder_path = \"processed_docs\"  # Ensure the path matches your folder structure\ndocuments = _read_documents_from_folder(folder_path)\ndoc_dict = {doc[0]: doc[1] for doc in documents}\n# Initialize BM25 Retriever\nbm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"final_all-MiniLM-L6-v2\"\nMODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "documents = _read_documents_from_folder(folder_path)\ndoc_dict = {doc[0]: doc[1] for doc in documents}\n# Initialize BM25 Retriever\nbm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"final_all-MiniLM-L6-v2\"\nMODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # adjust as needed",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "doc_dict",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "doc_dict = {doc[0]: doc[1] for doc in documents}\n# Initialize BM25 Retriever\nbm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"final_all-MiniLM-L6-v2\"\nMODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # adjust as needed\n    encode_kwargs={\"normalize_embeddings\": True}",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "bm25_retriever",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "bm25_retriever = BM25Retriever(documents)\n# Initialize Vector Retrieval\nVECTOR_DB_DIR = \"final_all-MiniLM-L6-v2\"\nMODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # adjust as needed\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "VECTOR_DB_DIR",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "VECTOR_DB_DIR = \"final_all-MiniLM-L6-v2\"\nMODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # adjust as needed\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,\n    embedding_function=embedding_model",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "MODEL_NAME",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # adjust as needed\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,\n    embedding_function=embedding_model\n)",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "embedding_model",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "embedding_model = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={\"device\": \"mps\"},  # adjust as needed\n    encode_kwargs={\"normalize_embeddings\": True}\n)\nvectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,\n    embedding_function=embedding_model\n)\nvector_retriever = vectorstore.as_retriever(",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "vectorstore = Chroma(\n    persist_directory=VECTOR_DB_DIR,\n    embedding_function=embedding_model\n)\nvector_retriever = vectorstore.as_retriever(\n    # we will control top_n later\n    search_type=\"similarity\", search_kwargs={\"k\": top_n_initial}\n)\n# Step 3: Initialize the LLM judge using GPT-4 (unchanged)\nclass RelevanceScore(BaseModel):",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "vector_retriever",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "vector_retriever = vectorstore.as_retriever(\n    # we will control top_n later\n    search_type=\"similarity\", search_kwargs={\"k\": top_n_initial}\n)\n# Step 3: Initialize the LLM judge using GPT-4 (unchanged)\nclass RelevanceScore(BaseModel):\n    score: int = Field(\n        description=\"The relevance score of the retrieval from 0 to 10\")\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "model = ChatOpenAI(model=\"gpt-4o-mini\")\nstructured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "structured_llm",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "structured_llm = model.with_structured_output(RelevanceScore)\ndef query_llm(content, question):\n    \"\"\"Query the LLM to rate the relevance of a document.\"\"\"\n    prompt = f\"\"\"\nYou are tasked with evaluating the relevance of a pull request (PR) document to a given user query. \nThe goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. \nEach PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.\nRate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:\n1. **Evaluation Criteria**:\n    - Assess how specifically and accurately the PR addresses or provides relevant information for the query.",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "cross_encoder_model",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "cross_encoder_model = CrossEncoder(\"BAAI/bge-reranker-base\")\n# Define thresholds for relevance\nRELEVANCE_THRESHOLD = 7\nresults = {}\noverall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "RELEVANCE_THRESHOLD",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "RELEVANCE_THRESHOLD = 7\nresults = {}\noverall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []\n}",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "results = {}\noverall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []\n}\nfor [file_name_query, question] in questions:",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "overall_metrics",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "overall_metrics = {\n    \"Precision\": [],\n    \"MAP\": [],\n    \"MRR\": [],\n    \"nDCG\": [],\n    \"average_score\": [],\n    \"max_score\": []\n}\nfor [file_name_query, question] in questions:\n    print(\"Evaluating: \", question)",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "results[\"overall_averages\"]",
        "kind": 5,
        "importPath": "6_reranking.without_reranking",
        "description": "6_reranking.without_reranking",
        "peekOfCode": "results[\"overall_averages\"] = {\n    metric: np.mean(values) for metric, values in overall_metrics.items()\n}\nwith open(\"evaluation_results_ensemble_no_reranking.json\", \"w\") as json_file:\n    json.dump(results, json_file, indent=4)\nprint(\"Results have been saved to evaluation_results_ensemble.json.\")",
        "detail": "6_reranking.without_reranking",
        "documentation": {}
    },
    {
        "label": "prompt_with_default",
        "kind": 2,
        "importPath": "DEMO (for hanjin).index",
        "description": "DEMO (for hanjin).index",
        "peekOfCode": "def prompt_with_default(prompt, default):\n    \"\"\"\n    Prompt the user for input, showing a default value.\n    Args:\n        prompt (str): The question to display to the user.\n        default (str): The default value to use if the user provides no input.\n    Returns:\n        str: User's input or the default value if no input is provided.\n    \"\"\"\n    user_input = input(f\"{prompt} (Default: {default}): \").strip()",
        "detail": "DEMO (for hanjin).index",
        "documentation": {}
    },
    {
        "label": "confirm_with_default",
        "kind": 2,
        "importPath": "DEMO (for hanjin).index",
        "description": "DEMO (for hanjin).index",
        "peekOfCode": "def confirm_with_default(prompt, default):\n    \"\"\"\n    Prompt the user with a yes/no question, showing a default value.\n    Args:\n        prompt (str): The question to display to the user.\n        default (str): Default response ('y' or 'n').\n    Returns:\n        bool: True if the user confirms ('y'), False otherwise.\n    \"\"\"\n    default = default.lower()",
        "detail": "DEMO (for hanjin).index",
        "documentation": {}
    },
    {
        "label": "load_pr_data",
        "kind": 2,
        "importPath": "DEMO (for hanjin).index",
        "description": "DEMO (for hanjin).index",
        "peekOfCode": "def load_pr_data(pr_folder):\n    \"\"\"\n    Load PR data from the specified folder.\n    Args:\n        pr_folder (str): Path to the folder containing PR data in .txt format.\n    Returns:\n        list[Document]: A list of Document objects, each containing the content of a PR and metadata.\n    \"\"\"\n    pr_documents = []\n    for file in os.listdir(pr_folder):",
        "detail": "DEMO (for hanjin).index",
        "documentation": {}
    },
    {
        "label": "query_prs",
        "kind": 2,
        "importPath": "DEMO (for hanjin).index",
        "description": "DEMO (for hanjin).index",
        "peekOfCode": "def query_prs(query):\n    \"\"\"\n    Retrieve and display relevant PRs for a given query.\n    Args:\n        query (str): User's input query.\n    Returns:\n        list[dict]: List of dictionaries containing retrieved PR content and metadata.\n    \"\"\"\n    print(f\"Querying PRs for: {query}\")\n    retrieved_docs = retriever.invoke(query)  # Retrieve relevant documents",
        "detail": "DEMO (for hanjin).index",
        "documentation": {}
    },
    {
        "label": "USE_LOCAL_DB",
        "kind": 5,
        "importPath": "DEMO (for hanjin).index",
        "description": "DEMO (for hanjin).index",
        "peekOfCode": "USE_LOCAL_DB = confirm_with_default(\n    \"Do you want to use the existing local database? If this is your first run, answer 'n'\",\n    \"n\"\n)\n# Prompt for directory paths and model name with default values\nPR_FOLDER = prompt_with_default(\n    \"Enter the path to the folder containing processed PR data files\",\n    \"processed_docs\"\n)\nVECTOR_DB_DIR = prompt_with_default(",
        "detail": "DEMO (for hanjin).index",
        "documentation": {}
    },
    {
        "label": "PR_FOLDER",
        "kind": 5,
        "importPath": "DEMO (for hanjin).index",
        "description": "DEMO (for hanjin).index",
        "peekOfCode": "PR_FOLDER = prompt_with_default(\n    \"Enter the path to the folder containing processed PR data files\",\n    \"processed_docs\"\n)\nVECTOR_DB_DIR = prompt_with_default(\n    \"Enter the directory path to save/load the vector database\",\n    \"1_gte_small_demo_hanjin\"\n)\nMODEL_NAME = prompt_with_default(\n    \"Enter the model name for HuggingFace embeddings\",",
        "detail": "DEMO (for hanjin).index",
        "documentation": {}
    },
    {
        "label": "VECTOR_DB_DIR",
        "kind": 5,
        "importPath": "DEMO (for hanjin).index",
        "description": "DEMO (for hanjin).index",
        "peekOfCode": "VECTOR_DB_DIR = prompt_with_default(\n    \"Enter the directory path to save/load the vector database\",\n    \"1_gte_small_demo_hanjin\"\n)\nMODEL_NAME = prompt_with_default(\n    \"Enter the model name for HuggingFace embeddings\",\n    \"thenlper/gte-small\"\n)\ndef load_pr_data(pr_folder):\n    \"\"\"",
        "detail": "DEMO (for hanjin).index",
        "documentation": {}
    },
    {
        "label": "MODEL_NAME",
        "kind": 5,
        "importPath": "DEMO (for hanjin).index",
        "description": "DEMO (for hanjin).index",
        "peekOfCode": "MODEL_NAME = prompt_with_default(\n    \"Enter the model name for HuggingFace embeddings\",\n    \"thenlper/gte-small\"\n)\ndef load_pr_data(pr_folder):\n    \"\"\"\n    Load PR data from the specified folder.\n    Args:\n        pr_folder (str): Path to the folder containing PR data in .txt format.\n    Returns:",
        "detail": "DEMO (for hanjin).index",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "DEMO (for hanjin).index",
        "description": "DEMO (for hanjin).index",
        "peekOfCode": "retriever = vectorstore.as_retriever(\n    search_type=\"similarity\", search_kwargs={\"k\": 10})\ndef query_prs(query):\n    \"\"\"\n    Retrieve and display relevant PRs for a given query.\n    Args:\n        query (str): User's input query.\n    Returns:\n        list[dict]: List of dictionaries containing retrieved PR content and metadata.\n    \"\"\"",
        "detail": "DEMO (for hanjin).index",
        "documentation": {}
    },
    {
        "label": "BM25Retriever",
        "kind": 6,
        "importPath": "UI DEMO.retriever",
        "description": "UI DEMO.retriever",
        "peekOfCode": "class BM25Retriever:\n    def __init__(self, documents):\n        \"\"\"\n        Initialize BM25 Retriever using rank-bm25 library.\n        :param documents: List of documents (each document is a string).\n        \"\"\"\n        self.tokenized_documents = [doc.split()\n                                    for [file_name, doc] in documents]\n        self.bm25 = BM25Okapi(self.tokenized_documents)\n    def retrieve(self, query, top_n=5):",
        "detail": "UI DEMO.retriever",
        "documentation": {}
    },
    {
        "label": "EnsembleRetriever",
        "kind": 6,
        "importPath": "UI DEMO.retriever",
        "description": "UI DEMO.retriever",
        "peekOfCode": "class EnsembleRetriever:\n    def __init__(self, folder_path, vector_db_dir, model_name, device=\"mps\"):\n        # Load environment variables\n        load_dotenv()\n        # Initialize documents\n        self.folder_path = folder_path\n        self.documents = _read_documents_from_folder(folder_path)\n        self.doc_dict = {doc[0]: doc[1] for doc in self.documents}\n        # Initialize BM25 Retriever\n        self.bm25_retriever = BM25Retriever(self.documents)",
        "detail": "UI DEMO.retriever",
        "documentation": {}
    },
    {
        "label": "call_llm",
        "kind": 2,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "def call_llm(system_prompt, user_prompt, model=\"gpt-4o\", temperature=0.7):\n    \"\"\"Generic LLM call.\"\"\"\n    response = openai.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        temperature=temperature\n    )",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "prompt_clean_and_expand",
        "kind": 2,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "def prompt_clean_and_expand(query):\n    system_prompt = PROMPT_TEMPLATES[\"query_refinement\"][\"system\"]\n    user_prompt = PROMPT_TEMPLATES[\"query_refinement\"][\"user\"].format(\n        query=query)\n    return call_llm(system_prompt, user_prompt, model=\"gpt-4o\", temperature=0)\ndef summarize_document(query, content):\n    system_prompt = PROMPT_TEMPLATES[\"summarization\"][\"system\"]\n    user_prompt = PROMPT_TEMPLATES[\"summarization\"][\"user\"].format(\n        query=query, content=content)\n    return call_llm(system_prompt, user_prompt, model=\"gpt-4o\", temperature=0)",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "summarize_document",
        "kind": 2,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "def summarize_document(query, content):\n    system_prompt = PROMPT_TEMPLATES[\"summarization\"][\"system\"]\n    user_prompt = PROMPT_TEMPLATES[\"summarization\"][\"user\"].format(\n        query=query, content=content)\n    return call_llm(system_prompt, user_prompt, model=\"gpt-4o\", temperature=0)\ndef generate_final_answer(query, summaries):\n    context = \"\\n\\n\".join(\n        [f\"Document {i+1} summary:\\n{summ}\" for i, summ in enumerate(summaries)])\n    system_prompt = PROMPT_TEMPLATES[\"final_answer\"][\"system\"]\n    user_prompt = PROMPT_TEMPLATES[\"final_answer\"][\"user\"].format(",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "generate_final_answer",
        "kind": 2,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "def generate_final_answer(query, summaries):\n    context = \"\\n\\n\".join(\n        [f\"Document {i+1} summary:\\n{summ}\" for i, summ in enumerate(summaries)])\n    system_prompt = PROMPT_TEMPLATES[\"final_answer\"][\"system\"]\n    user_prompt = PROMPT_TEMPLATES[\"final_answer\"][\"user\"].format(\n        query=query, context=context)\n    return call_llm(system_prompt, user_prompt, model=\"gpt-4o\", temperature=0)\ndef extract_pr_info(text):\n    pr_number_pattern = r\"Pull Request Number:\\s*(\\d+)\"\n    pr_title_pattern = r\"Title:\\s*(.*)\"",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "extract_pr_info",
        "kind": 2,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "def extract_pr_info(text):\n    pr_number_pattern = r\"Pull Request Number:\\s*(\\d+)\"\n    pr_title_pattern = r\"Title:\\s*(.*)\"\n    pr_url_pattern = r\"URL:\\s*(https?://[^\\s]+)\"\n    pr_num = re.search(pr_number_pattern, text)\n    pr_title = re.search(pr_title_pattern, text)\n    pr_url = re.search(pr_url_pattern, text)\n    if pr_num and pr_url:\n        return pr_num.group(1), (pr_title.group(1) if pr_title else \"No Title Found\"), pr_url.group(1)\n    return None",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "add_citations_to_answer",
        "kind": 2,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "def add_citations_to_answer(final_answer, references):\n    refs_str = \"\\n\".join(\n        [f\"PR Number: {ref['pr_number']}, Title: {ref['title']}, URL: {ref['url']}\" for ref in references]\n    )\n    system_prompt = \"\"\"You are a post-processing assistant that adds citations to a previously generated answer.\nYou are given a final answer text and a list of PR references. Insert citations in the answer where relevant.\nEven if it is slightly relevant or little relevant, YOU SHOULD STILL PUT THE CITATION.\nIdeally every line of your answer or every information should be cited by citation.\nRules:\n- Use the format: <citation pr_number=\"...\" title=\"...\" url=\"...\" />.",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "parse_citations",
        "kind": 2,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "def parse_citations(annotated_answer):\n    pattern = r'<citation\\s+pr_number=\"([^\"]+)\"\\s+title=\"([^\"]+)\"\\s+url=\"([^\"]+)\"\\s*/>'\n    matches = re.findall(pattern, annotated_answer)\n    citations = []\n    for pr_number, title, url in matches:\n        citations.append({\"pr_number\": pr_number, \"title\": title, \"url\": url})\n    return citations\ndef replace_citations_with_links(text):\n    pattern = r'<citation\\s+pr_number=\"([^\"]+)\"\\s+title=\"([^\"]+)\"\\s+url=\"([^\"]+)\"\\s*/>'\n    def repl(match):",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "replace_citations_with_links",
        "kind": 2,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "def replace_citations_with_links(text):\n    pattern = r'<citation\\s+pr_number=\"([^\"]+)\"\\s+title=\"([^\"]+)\"\\s+url=\"([^\"]+)\"\\s*/>'\n    def repl(match):\n        pr_number = match.group(1)\n        title = match.group(2)\n        url = match.group(3)\n        return f\"[(PR #{pr_number})]({url})\"\n    return re.sub(pattern, repl, text)\n# Streamlit Workflow\nuser_query = st.text_input(",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "openai.api_key",
        "kind": 5,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n# Explanation of GitGlimpse (for context, not part of the app logic):\n# GitGlimpse is a specialized search engine designed to help developers quickly\n# locate and understand relevant pull requests (PRs) from vast repositories.\n# By leveraging Retrieval-Augmented Generation (RAG) and LLM-powered summarization,\n# GitGlimpse transforms PR data into actionable insights. Developers can input\n# queries about specific tasks or features, and GitGlimpse retrieves and ranks PRs\n# based on relevance, providing concise summaries and context. This reduces time\n# spent searching through unrelated PRs, accelerating feature development and\n# enhancing collaboration.",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "folder_path",
        "kind": 5,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "folder_path = \"processed_docs\"  # adjust as needed\nvector_db_dir = \"vector_db\"\nmodel_name = \"thenlper/gte-small\"\nretriever = EnsembleRetriever(\n    folder_path, vector_db_dir, model_name=model_name, device=\"cpu\")\nst.set_page_config(page_title=\"RAG Demo\", page_icon=\"\", layout=\"wide\")\nst.title(\"RAG Demo: Retrieval Augmented Generation\")\n# Explanation / Instructions\nst.markdown(\"\"\"\n### Instructions",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "vector_db_dir",
        "kind": 5,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "vector_db_dir = \"vector_db\"\nmodel_name = \"thenlper/gte-small\"\nretriever = EnsembleRetriever(\n    folder_path, vector_db_dir, model_name=model_name, device=\"cpu\")\nst.set_page_config(page_title=\"RAG Demo\", page_icon=\"\", layout=\"wide\")\nst.title(\"RAG Demo: Retrieval Augmented Generation\")\n# Explanation / Instructions\nst.markdown(\"\"\"\n### Instructions\n1. Type your question into the input box below.",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "model_name = \"thenlper/gte-small\"\nretriever = EnsembleRetriever(\n    folder_path, vector_db_dir, model_name=model_name, device=\"cpu\")\nst.set_page_config(page_title=\"RAG Demo\", page_icon=\"\", layout=\"wide\")\nst.title(\"RAG Demo: Retrieval Augmented Generation\")\n# Explanation / Instructions\nst.markdown(\"\"\"\n### Instructions\n1. Type your question into the input box below.\n2. The system will:",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "retriever = EnsembleRetriever(\n    folder_path, vector_db_dir, model_name=model_name, device=\"cpu\")\nst.set_page_config(page_title=\"RAG Demo\", page_icon=\"\", layout=\"wide\")\nst.title(\"RAG Demo: Retrieval Augmented Generation\")\n# Explanation / Instructions\nst.markdown(\"\"\"\n### Instructions\n1. Type your question into the input box below.\n2. The system will:\n   - Clean and expand your query using an LLM.",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "PROMPT_TEMPLATES",
        "kind": 5,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "PROMPT_TEMPLATES = {\n    \"query_refinement\": {\n        \"system\": \"\"\"You are a senior developer at a large company. Your job is to refine and expand any given user query related to a software codebase or feature. The user is a newcomer developer who needs more specific, detailed, and actionable queries to find the right documents. Your refined query should:\n- Add relevant keywords or technologies that might be involved.\n- Ensure the query is well-targeted so that the retrieval system can find the most pertinent PRs or code references. (there is no need to mention \"check for PR\" as everything in the database is PR already. You should focus on the component name, concepts, tech, or terms)\nYour query will be passed to search engine or vector retrieval. \nPriortize on querying the component name.\nYour response must immediately begin with the query - Do not put \"Relevant Query:\" at the beginning.\n\"\"\",\n        \"user\": \"\"\"Original user query: {query}\\nPlease refine and expand this query to improve document retrieval.\"\"\"",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "user_query",
        "kind": 5,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "user_query = st.text_input(\n    \"Ask a question:\", \"How to add virtualization support to NextUI component?\")\n# I want to solve flickering issue in next ui button. How can i investigate this? describe a good investigation workflow.\nrun_button = st.button(\"Run\")\nif run_button:\n    st.subheader(\"Chain of Thought\")\n    # Step 1: Query Refinement\n    st.markdown(\"**1 Step 1: Query Refinement**\")\n    refined_query = prompt_clean_and_expand(user_query)\n    st.write(\"**Refined Query:**\", refined_query)",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    },
    {
        "label": "run_button",
        "kind": 5,
        "importPath": "UI DEMO.streamlit_app",
        "description": "UI DEMO.streamlit_app",
        "peekOfCode": "run_button = st.button(\"Run\")\nif run_button:\n    st.subheader(\"Chain of Thought\")\n    # Step 1: Query Refinement\n    st.markdown(\"**1 Step 1: Query Refinement**\")\n    refined_query = prompt_clean_and_expand(user_query)\n    st.write(\"**Refined Query:**\", refined_query)\n    # Step 2: Retrieve Relevant Documents\n    st.markdown(\"**2 Step 2: Retrieve Documents**\")\n    retrieved_docs = retriever.get_documents(refined_query)",
        "detail": "UI DEMO.streamlit_app",
        "documentation": {}
    }
]
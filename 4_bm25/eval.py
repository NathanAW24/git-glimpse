import os
import re
import numpy as np
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from pydantic import BaseModel, Field
import json

from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set OpenAI API Key
if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = input("Enter your OpenAI API key: ")

# Step 1: Define your questions for evaluation
questions = [
    "How do I create a new form on Angular v2?",
    "How do I create a new endpoint for UOB tenant?",
    "Where do I store the email templates for sending an email on UOB tenant?",
    "Where is the component for *.haml view stored?",
    "How do I modify the webhook payload sent to the backend?",
    "In which haml view should I put my new Angular v2 component for UOB landing page?",
    "How do I create a new representer to modify the retrieved data from DB?",
    "Where do I modify the CSS component for this haml view?",
    "How do I create a new route to redirect to a new Angular component page?",

    # one sentence, generated by chatgpt
    "How can I address the issue of missing focus state indicators (such as dotted borders) for navigation buttons in the search bar on Safari and IE11, ensuring that users can clearly see which element is focused when navigating with the keyboard? Additionally, are there any specific CSS techniques or workarounds needed to handle focus states in these browsers, particularly when dealing with non-standard behavior like invisible outlines?",
    "How should I adjust the spacing of a colon in relation to the sort text on the BNZ results page, and what CSS changes are necessary to achieve the correct positioning?",
    "How can I implement a feature to make toggle options bold when selected, ensure a calendar icon activates the date selection dropdown, and address UX issues related to displaying the number of children, while also ensuring compatibility across various browsers and devices, particularly focusing on fixing date picker interactions in Safari for iOS and desktop?",
    "How can I implement a rate-limiting mechanism to prevent signup spam by throttling requests based on IP address in our Rails application?",
    "How should I apply CSS changes to correctly support RTL (Right-To-Left) languages, such as Arabic, on the FAB Redeem hotel confirmation page, ensuring that the UI components are aligned and formatted correctly according to RTL standards?"


    "How can I remove a testing whitelist like Acme from the production configuration to prevent it from being used in production setups, and what testing steps should I follow to ensure it's correctly removed?",
    "How do we handle PaxId generation for GTA bookings to ensure that bookings with one adult per room are processed correctly and avoid pending status issues?",
    "How can I implement a unified maintenance page UI across multiple whitelabel brands with a global setting toggle for enabling or disabling maintenance mode?",
    "How can I update the PDF template in our email confirmations to include dynamic credit card descriptor messages based on the payment method, similar to how it's done in email bodies?",
    "How do I implement translation updates for a signup widget on the homepage across multiple locales, including handling any necessary reversions or adjustments in existing translation files?"

    # very concise, generated by chatgpt
    "How do I migrate an AngularJS component to Angular v2 for a checkout banner?",
    "How to fix CSS layout issues in hotel details page?",
    "How can I add verbose error logs to price validation in our system?",
    "How to calculate and update the default ascenda_margin for M2020?",
    "How to handle icon format inconsistencies causing 403 errors?"
]

# Step 2: Load vector database
VECTOR_DB_DIR = "vector_db"
MODEL_NAME = "thenlper/gte-small"
# Use OpenAI embeddings if applicable
embedding_model = HuggingFaceEmbeddings(
    model_name=MODEL_NAME,
    # Ensure the device matches the previous setup
    model_kwargs={"device": "mps"},
    encode_kwargs={"normalize_embeddings": True}
)
vectorstore = Chroma(
    persist_directory=VECTOR_DB_DIR,
    embedding_function=embedding_model
)
retriever = vectorstore.as_retriever(
    search_type="similarity", search_kwargs={"k": 10})

# Step 3: Initialize the LLM judge using GPT-4


class RelevanceScore(BaseModel):
    score: int = Field(
        description="The relevance score of the retrieval from 0 to 10")


model = ChatOpenAI(model="gpt-4o-mini")
structured_llm = model.with_structured_output(RelevanceScore)


def query_llm(content, question):
    """Query the LLM to rate the relevance of a document."""
    prompt = f"""
You are tasked with evaluating the relevance of a pull request (PR) document to a given user query. 
The goal is to assess how well this PR aligns with the query in the context of building a search engine for a Retrieval-Augmented Generation (RAG) application. 
Each PR is a chunk of data stored in a text file and includes metadata, descriptions, and file changes.

Rate the relevance of the PR document to the user query on a scale from 0 to 10. Follow these specific instructions:

1. **Evaluation Criteria**:
    - Assess how specifically and accurately the PR addresses or provides relevant information for the query.
    - Consider whether the PR contains meaningful file changes, code implementations, or documentation that directly answer the query.

2. **Scoring Scale**:
    - **10**: The PR perfectly matches the query, providing complete implementation, precise details, and thorough documentation.
        Example: the PR includes the full implementation of the login page, its components, routing, and styling.
    - **9**: The PR is highly relevant, providing most of the implementation but missing minor details.
        Example: the PR includes the components and routing but lacks styling or error handling.
    - **8**: The PR is very relevant, addressing the query but with noticeable gaps in coverage or precision.
        Example: the PR includes partial implementation but lacks comprehensive testing or edge-case handling.
    - **7**: The PR is relevant but may lack significant aspects needed to fully answer the query.
        Example: the PR discusses adding authentication components but does not include the actual login page.
    - **6**: The PR is moderately relevant, providing tangential or limited information related to the query.
        Example: the PR includes code for authentication but focuses on unrelated components.
    - **5**: The PR is partially relevant but contains significant gaps or focuses on adjacent topics.
        Example: the PR references form controls but lacks any mention of a login page.
    - **4**: The PR is slightly relevant, with minimal alignment to the query.
        Example: the PR includes changes to Angular components unrelated to login functionality.
    - **3**: The PR is poorly relevant, with only marginal connection to the query.
        Example: the PR mentions Angular v2 in passing but focuses on unrelated features.
    - **2**: The PR is barely relevant, with a negligible connection to the query.
        Example: the PR is about an entirely different module but uses Angular v2.
    - **1**: The PR is irrelevant, with almost no connection to the query.
        Example: the PR is about backend features with no mention of Angular or login pages.
    - **0**: The PR is completely off-topic, providing no useful information for the query.
        Example: the PR is about a Python backend service unrelated to the query.

3. **Response Format**:
    - Your response must be a single integer from 0 to 10.
    - Do **not** include any text, explanation, or additional characters.

User Query: "{question}"
Pull Request Document:
{content}

Respond with the relevance score only (e.g., 7).
"""
    response = structured_llm.invoke(prompt)
    print(response.score)
    # try:
    #     score = int(re.search(r"\d+", response).group())
    # except (AttributeError, ValueError):
    #     score = 0  # Default score if parsing fails
    return response.score


# Define thresholds for relevance
RELEVANCE_THRESHOLD = 7

# Step 4: Evaluate each question
results = {}
overall_metrics = {
    "Precision": [],
    "MAP": [],
    "MRR": [],
    "nDCG": []
}

for question in questions:
    # print("Evaluating: ", question)
    retrieved_docs = retriever.invoke(question)
    doc_scores = []
    for doc in retrieved_docs:
        content = doc.page_content
        score = query_llm(content, question)
        doc_scores.append(score)

    # Normalize scores to binary relevance (1 for relevant, 0 for not relevant)
    binary_relevance = [1 if score >=
                        RELEVANCE_THRESHOLD else 0 for score in doc_scores]

    # Calculate Precision
    relevant_count = sum(binary_relevance)
    precision = relevant_count / len(doc_scores) if doc_scores else 0

    # Calculate Average Precision
    precision_values = [
        sum(binary_relevance[:i + 1]) / (i + 1)
        for i in range(len(binary_relevance))
        if binary_relevance[i] == 1
    ]
    average_precision = sum(precision_values) / \
        relevant_count if relevant_count > 0 else 0

    # Add MAP (Mean Average Precision)
    map_score = average_precision  # For a single query, MAP equals Average Precision
    overall_metrics["MAP"].append(map_score)

    # Calculate MRR
    mrr = 0
    for i, is_relevant in enumerate(binary_relevance):
        if is_relevant == 1:
            mrr = 1 / (i + 1)
            break

    # Calculate cumulative score and nDCG
    cumulative_score = sum(doc_scores)
    dcg = sum(binary_relevance[idx] / np.log2(idx + 2)
              for idx in range(len(binary_relevance)))
    idcg = sum(1 / np.log2(idx + 2) for idx in range(relevant_count))
    ndcg = dcg / idcg if idcg > 0 else 0

    # Add metrics for this query
    results[question] = {
        "scores": doc_scores,
        "cumulative_score": cumulative_score,
        "Precision": precision,
        "MAP": map_score,
        "MRR": mrr,
        "nDCG": ndcg,
    }

    # Aggregate overall metrics
    overall_metrics["Precision"].append(precision)
    overall_metrics["MRR"].append(mrr)
    overall_metrics["nDCG"].append(ndcg)

# Compute overall averages
results["overall_averages"] = {
    metric: np.mean(values) for metric, values in overall_metrics.items()
}

# Save results to JSON files
with open("evaluation_results_v2.json", "w") as json_file:
    json.dump(results, json_file, indent=4)

print("Results have been saved to evaluation_results.json.")
print("Overall metrics have been saved to overall_metrics.json.")
